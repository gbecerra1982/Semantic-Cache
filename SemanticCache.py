import os
import time
import json
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import numpy as np
from openai import AzureOpenAI
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential
import faiss
import pickle

# FunciÃ³n para solicitar parÃ¡metros
def get_parameters():
    """Solicita los parÃ¡metros de configuraciÃ³n con valores predeterminados."""
    print("ğŸ”§ CONFIGURACIÃ“N DE AZURE AI FOUNDRY")
    print("=" * 60)
    print("Presiona Enter para usar los valores predeterminados\n")
    
    # SDK a usar
    print("Selecciona el SDK a usar:")
    print("1. Azure OpenAI SDK con endpoint de OpenAI (predeterminado)")
    print("2. Azure OpenAI SDK con endpoint de Foundry")
    print("3. Azure AI Foundry SDK (experimental)")
    sdk_choice = input("OpciÃ³n [1]: ").strip() or "1"
    
    if sdk_choice == "3":
        # ConfiguraciÃ³n para Azure AI Foundry SDK
        endpoint = input(f"Azure AI Foundry endpoint [{DEFAULT_FOUNDRY_ENDPOINT}]: ").strip() or DEFAULT_FOUNDRY_ENDPOINT
        api_key = input(f"API Key [{DEFAULT_API_KEY[:20]}...]: ").strip() or DEFAULT_API_KEY
        
        # Deployment names para Foundry
        gpt_deployment = input(f"GPT Deployment name [{DEFAULT_GPT_DEPLOYMENT}]: ").strip() or DEFAULT_GPT_DEPLOYMENT
        embedding_deployment = input(f"Embedding Deployment name [{DEFAULT_EMBEDDING_DEPLOYMENT}]: ").strip() or DEFAULT_EMBEDDING_DEPLOYMENT
        
        return {
            'use_foundry': True,
            'endpoint': endpoint,
            'api_key': api_key,
            'gpt_deployment': gpt_deployment,
            'embedding_deployment': embedding_deployment,
            'api_version': None
        }
    elif sdk_choice == "2":
        # ConfiguraciÃ³n para Azure OpenAI SDK con endpoint de Foundry
        endpoint = input(f"Azure OpenAI endpoint [{DEFAULT_OPENAI_ENDPOINT}]: ").strip() or DEFAULT_OPENAI_ENDPOINT
        api_key = input(f"API Key [{DEFAULT_API_KEY[:20]}...]: ").strip() or DEFAULT_API_KEY
        api_version = input(f"API Version [{DEFAULT_API_VERSION}]: ").strip() or DEFAULT_API_VERSION
        
        # Deployment names
        gpt_deployment = input(f"GPT Deployment name [{DEFAULT_GPT_DEPLOYMENT}]: ").strip() or DEFAULT_GPT_DEPLOYMENT
        embedding_deployment = input(f"Embedding Deployment name [{DEFAULT_EMBEDDING_DEPLOYMENT}]: ").strip() or DEFAULT_EMBEDDING_DEPLOYMENT
        
        return {
            'use_foundry': False,
            'endpoint': endpoint,
            'api_key': api_key,
            'gpt_deployment': gpt_deployment,
            'embedding_deployment': embedding_deployment,
            'api_version': api_version
        }
    else:
        # ConfiguraciÃ³n estÃ¡ndar para Azure OpenAI
        endpoint = input(f"Azure OpenAI endpoint [Tu endpoint]: ").strip()
        api_key = input(f"API Key: ").strip()
        api_version = input(f"API Version [{DEFAULT_API_VERSION}]: ").strip() or DEFAULT_API_VERSION
        
        # Deployment names
        gpt_deployment = input(f"GPT Deployment name [{DEFAULT_GPT_DEPLOYMENT}]: ").strip() or DEFAULT_GPT_DEPLOYMENT
        embedding_deployment = input(f"Embedding Deployment name [{DEFAULT_EMBEDDING_DEPLOYMENT}]: ").strip() or DEFAULT_EMBEDDING_DEPLOYMENT
        
        return {
            'use_foundry': False,
            'endpoint': endpoint,
            'api_key': api_key,
            'gpt_deployment': gpt_deployment,
            'embedding_deployment': embedding_deployment,
            'api_version': api_version
        }

# Valores predeterminados
DEFAULT_FOUNDRY_ENDPOINT = "https://foundry-proyecto1.services.ai.azure.com/api/projects/myFirstProject"
DEFAULT_OPENAI_ENDPOINT = "https://foundry-proyecto1.openai.azure.com/"
DEFAULT_API_KEY = "44E5Jtv6MfBOtx7565zFDoGXV8hTHeUrokBk7DdArzC69NAFC7ZxJQQJ99BFAC4f1cMXJ3w3AAAAACOGVYsT"
DEFAULT_API_VERSION = "2024-02-01"
DEFAULT_GPT_DEPLOYMENT = "gpt-4.1"
DEFAULT_EMBEDDING_DEPLOYMENT = "text-embedding"

# ConfiguraciÃ³n de cachÃ©
CACHE_FILE = "semantic_cache.pkl"
INDEX_FILE = "semantic_index.faiss"
SIMILARITY_THRESHOLD = 0.85  # Umbral de similitud para considerar un hit de cachÃ©

class SemanticCache:
    def __init__(self, embedding_dimension: int = None, config: Dict = None):
        """Inicializa la cachÃ© semÃ¡ntica con FAISS."""
        self.embedding_dimension = embedding_dimension
        self.index = None  # Se inicializarÃ¡ despuÃ©s de conocer la dimensiÃ³n
        self.cache: Dict[int, Dict] = {}
        self.cache_hits = 0
        self.cache_misses = 0
        self.config = config or {}
        self._initialized = False
        
    def _initialize_index(self, dimension: int):
        """Inicializa el Ã­ndice FAISS con la dimensiÃ³n correcta."""
        if not self._initialized:
            self.embedding_dimension = dimension
            self.index = faiss.IndexFlatL2(dimension)
            self._initialized = True
            print(f"ğŸ“ Ãndice inicializado con dimensiÃ³n: {dimension}")
    
    def get_embedding(self, text: str, client) -> List[float]:
        """Genera el embedding para un texto usando Azure OpenAI o Foundry."""
        if self.config.get('use_foundry'):
            # Para Foundry, necesitamos usar el cliente de embeddings especÃ­fico
            # Por ahora, usaremos el cliente OpenAI ya que Foundry usa la misma API
            if hasattr(client, 'embeddings'):
                response = client.embeddings.create(
                    model=self.config['embedding_deployment'],
                    input=text
                )
            else:
                # Si es el cliente de Foundry ChatCompletions, crear uno de OpenAI para embeddings
                openai_client = AzureOpenAI(
                    azure_endpoint=self.config['endpoint'].replace('/api/projects/myFirstProject', ''),
                    api_key=self.config['api_key'],
                    api_version="2024-02-01"
                )
                response = openai_client.embeddings.create(
                    model=self.config['embedding_deployment'],
                    input=text
                )
        else:
            response = client.embeddings.create(
                model=self.config['embedding_deployment'],
                input=text
            )
        
        embedding = response.data[0].embedding
        
        # Inicializar el Ã­ndice si es necesario
        if not self._initialized:
            self._initialize_index(len(embedding))
            
        return embedding
    
    def cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Calcula la similitud del coseno entre dos vectores."""
        vec1 = vec1.flatten()
        vec2 = vec2.flatten()
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        return dot_product / (norm1 * norm2)
    
    def search(self, query_embedding: List[float], k: int = 5) -> Tuple[List[float], List[int]]:
        """Busca los k embeddings mÃ¡s similares en el Ã­ndice."""
        query_vector = np.array([query_embedding]).astype('float32')
        distances, indices = self.index.search(query_vector, min(k, self.index.ntotal))
        
        # Convertir distancias L2 a similitudes del coseno
        similarities = []
        for i, idx in enumerate(indices[0]):
            if idx != -1:  # FAISS retorna -1 para resultados no vÃ¡lidos
                stored_embedding = self.index.reconstruct(int(idx))
                similarity = self.cosine_similarity(query_vector, stored_embedding)
                similarities.append(similarity)
            else:
                similarities.append(0.0)
                
        return similarities, indices[0].tolist()
    
    def get(self, prompt: str, client) -> Optional[str]:
        """Busca una respuesta en cachÃ© para el prompt dado."""
        if not self._initialized or self.index.ntotal == 0:
            self.cache_misses += 1
            return None
            
        # Generar embedding del prompt
        prompt_embedding = self.get_embedding(prompt, client)
        
        # Buscar los mÃ¡s similares
        similarities, indices = self.search(prompt_embedding, k=1)
        
        if similarities and similarities[0] > SIMILARITY_THRESHOLD:
            # Hit de cachÃ©
            self.cache_hits += 1
            cache_entry = self.cache[indices[0]]
            print(f"\nâœ… CACHE HIT! Similitud: {similarities[0]:.4f}")
            print(f"   Prompt original: {cache_entry['prompt'][:50]}...")
            return cache_entry['response']
        
        # Miss de cachÃ©
        self.cache_misses += 1
        return None
    
    def put(self, prompt: str, response: str, client):
        """Almacena una respuesta en cachÃ©."""
        # Generar embedding
        embedding = self.get_embedding(prompt, client)
        embedding_vector = np.array([embedding]).astype('float32')
        
        # AÃ±adir al Ã­ndice FAISS
        idx = self.index.ntotal
        self.index.add(embedding_vector)
        
        # Almacenar en el diccionario de cachÃ©
        self.cache[idx] = {
            'prompt': prompt,
            'response': response,
            'timestamp': datetime.now().isoformat(),
            'embedding': embedding
        }
        
        print(f"ğŸ’¾ Respuesta almacenada en cachÃ© (Ã­ndice: {idx})")
    
    def save(self):
        """Guarda la cachÃ© y el Ã­ndice en disco."""
        # Guardar el Ã­ndice FAISS
        faiss.write_index(self.index, INDEX_FILE)
        
        # Guardar el diccionario de cachÃ©
        with open(CACHE_FILE, 'wb') as f:
            pickle.dump(self.cache, f)
            
        print(f"ğŸ’¾ CachÃ© guardada: {self.index.ntotal} entradas")
    
    def load(self):
        """Carga la cachÃ© y el Ã­ndice desde disco."""
        try:
            # Verificar si ambos archivos existen
            if not os.path.exists(INDEX_FILE) or not os.path.exists(CACHE_FILE):
                print("âš ï¸  No se encontrÃ³ cachÃ© previa - iniciando nueva cachÃ©")
                return False
                
            # Cargar el Ã­ndice FAISS
            self.index = faiss.read_index(INDEX_FILE)
            self._initialized = True
            self.embedding_dimension = self.index.d
            
            # Cargar el diccionario de cachÃ©
            with open(CACHE_FILE, 'rb') as f:
                self.cache = pickle.load(f)
                
            print(f"ğŸ“‚ CachÃ© cargada: {self.index.ntotal} entradas (dimensiÃ³n: {self.embedding_dimension})")
            return True
        except Exception as e:
            print(f"âš ï¸  Error al cargar cachÃ©: {e}")
            print("   Iniciando nueva cachÃ©...")
            return False
    
    def get_stats(self) -> Dict:
        """Retorna estadÃ­sticas de la cachÃ©."""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        
        return {
            'total_entries': self.index.ntotal,
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'total_requests': total_requests
        }

def call_gpt_with_cache(prompt: str, client, cache: SemanticCache, config: Dict) -> str:
    """Llama a GPT con cachÃ© semÃ¡ntica."""
    # Buscar en cachÃ©
    cached_response = cache.get(prompt, client)
    
    if cached_response:
        return cached_response
    
    # Si no estÃ¡ en cachÃ©, llamar a GPT
    print("ğŸ¤– Llamando a GPT-4...")
    start_time = time.time()
    
    if config.get('use_foundry'):
        # Usar Azure AI Foundry SDK
        response = client.complete(
            messages=[
                SystemMessage(content="Eres un asistente Ãºtil."),
                UserMessage(content=prompt)
            ],
            temperature=0.7,
            max_tokens=500,
            model=config['gpt_deployment']
        )
        result = response.choices[0].message.content
    else:
        # Usar Azure OpenAI SDK
        response = client.chat.completions.create(
            model=config['gpt_deployment'],
            messages=[
                {"role": "system", "content": "Eres un asistente Ãºtil."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=500
        )
        result = response.choices[0].message.content
    
    elapsed_time = time.time() - start_time
    print(f"â±ï¸  Tiempo de respuesta: {elapsed_time:.2f}s")
    
    # Almacenar en cachÃ©
    cache.put(prompt, result, client)
    
    return result

def create_client(config: Dict):
    """Crea el cliente apropiado segÃºn la configuraciÃ³n."""
    if config.get('use_foundry'):
        print("ğŸ­ Usando Azure AI Foundry SDK")
        # Cliente para chat - usando el endpoint base sin el modelo en la URL
        chat_client = ChatCompletionsClient(
            endpoint=config['endpoint'],
            credential=AzureKeyCredential(config['api_key'])
        )
        # Cliente para embeddings (usando OpenAI SDK porque Foundry usa la misma API)
        # Extraer el endpoint base de OpenAI
        openai_endpoint = config['endpoint'].replace('services.ai.azure.com/api/projects/myFirstProject', 'openai.azure.com')
        embedding_client = AzureOpenAI(
            azure_endpoint=openai_endpoint,
            api_key=config['api_key'],
            api_version="2024-02-01"
        )
        return chat_client, embedding_client
    else:
        print("ğŸ”· Usando Azure OpenAI SDK")
        client = AzureOpenAI(
            azure_endpoint=config['endpoint'],
            api_key=config['api_key'],
            api_version=config['api_version']
        )
        return client, client

def run_tests():
    """Ejecuta pruebas de la cachÃ© semÃ¡ntica."""
    # Obtener configuraciÃ³n
    config = get_parameters()
    
    print("\nğŸš€ Iniciando pruebas de cachÃ© semÃ¡ntica...")
    print(f"ğŸ“ Endpoint: {config['endpoint']}")
    print(f"ğŸ¤– GPT Deployment: {config['gpt_deployment']}")
    print(f"ğŸ“Š Embedding Deployment: {config['embedding_deployment']}")
    print(f"ğŸ¯ Umbral de similitud: {SIMILARITY_THRESHOLD}")
    print("-" * 50)
    
    # Inicializar cliente
    chat_client, embedding_client = create_client(config)
    
    # Inicializar cachÃ©
    cache = SemanticCache(config=config)
    cache.load()  # Intentar cargar cachÃ© existente
    
    # Conjunto de pruebas con prompts similares
    test_prompts = [
        # Grupo 1: Preguntas sobre Python
        "Â¿CuÃ¡les son las mejores prÃ¡cticas para escribir cÃ³digo Python?",
        "Â¿QuÃ© son las best practices para programar en Python?",
        "Dame las mejores prÃ¡cticas de Python",
        
        # Grupo 2: Preguntas sobre IA
        "Â¿QuÃ© es el aprendizaje automÃ¡tico?",
        "ExplÃ­came quÃ© es machine learning",
        "Â¿Puedes explicar el aprendizaje automÃ¡tico?",
        
        # Grupo 3: Preguntas diferentes
        "Â¿CuÃ¡l es la capital de Francia?",
        "Â¿CÃ³mo se hace una pizza margherita?",
        "Â¿CuÃ¡les son los beneficios del ejercicio?",
        
        # Repetir algunas para probar cachÃ©
        "Â¿CuÃ¡les son las mejores prÃ¡cticas para escribir cÃ³digo Python?",
        "Â¿QuÃ© es el aprendizaje automÃ¡tico?",
    ]
    
    print("\nğŸ§ª EJECUTANDO PRUEBAS:\n")
    
    for i, prompt in enumerate(test_prompts, 1):
        print(f"\n{'='*60}")
        print(f"Prueba {i}/{len(test_prompts)}")
        print(f"Prompt: {prompt}")
        print("-" * 60)
        
        start_time = time.time()
        
        # Usar el cliente apropiado
        if config.get('use_foundry'):
            response = call_gpt_with_cache(prompt, chat_client, cache, config)
        else:
            response = call_gpt_with_cache(prompt, chat_client, cache, config)
        
        total_time = time.time() - start_time
        
        print(f"\nRespuesta: {response[:200]}...")
        print(f"â±ï¸  Tiempo total: {total_time:.2f}s")
        
        # Mostrar estadÃ­sticas actuales
        stats = cache.get_stats()
        print(f"\nğŸ“Š EstadÃ­sticas actuales:")
        print(f"   - Entradas en cachÃ©: {stats['total_entries']}")
        print(f"   - Cache hits: {stats['cache_hits']}")
        print(f"   - Cache misses: {stats['cache_misses']}")
        print(f"   - Hit rate: {stats['hit_rate']:.2%}")
    
    # Guardar cachÃ©
    cache.save()
    
    # Resumen final
    print(f"\n{'='*60}")
    print("ğŸ“ˆ RESUMEN FINAL:")
    final_stats = cache.get_stats()
    print(f"   - Total de consultas: {final_stats['total_requests']}")
    print(f"   - Cache hits: {final_stats['cache_hits']}")
    print(f"   - Cache misses: {final_stats['cache_misses']}")
    print(f"   - Hit rate final: {final_stats['hit_rate']:.2%}")
    print(f"   - Entradas en cachÃ©: {final_stats['total_entries']}")
    
    # Prueba adicional: buscar similitudes
    print(f"\n{'='*60}")
    print("ğŸ” ANÃLISIS DE SIMILITUDES:\n")
    
    test_query = "Â¿CuÃ¡les son las buenas prÃ¡cticas de programaciÃ³n en Python?"
    print(f"Query de prueba: {test_query}")
    
    # Usar el cliente de embeddings
    query_embedding = cache.get_embedding(test_query, embedding_client)
    similarities, indices = cache.search(query_embedding, k=5)
    
    print("\nTop 5 entradas mÃ¡s similares:")
    for i, (sim, idx) in enumerate(zip(similarities, indices)):
        if idx != -1 and idx in cache.cache:
            entry = cache.cache[idx]
            print(f"\n{i+1}. Similitud: {sim:.4f}")
            print(f"   Prompt: {entry['prompt']}")
            print(f"   Timestamp: {entry['timestamp']}")

if __name__ == "__main__":
    try:
        run_tests()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Prueba interrumpida por el usuario")
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()