# üöÄ Cach√© Sem√°ntico Optimizado para Azure OpenAI

Implementaci√≥n de cach√© sem√°ntico inteligente que reduce costos hasta un 90% y mejora el rendimiento hasta 20x mediante Azure API Management y Azure AI Foundry.

## üìã Tabla de Contenidos

- [Arquitectura del Sistema](#-arquitectura-del-sistema)
- [Caracter√≠sticas Principales](#-caracter√≠sticas-principales)
- [Implementaci√≥n desde Azure AI Foundry](#-implementaci√≥n-desde-azure-ai-foundry)
- [Configuraci√≥n de Pol√≠ticas](#-configuraci√≥n-de-pol√≠ticas)
- [Scripts de Prueba y Validaci√≥n](#-scripts-de-prueba-y-validaci√≥n)
- [Monitoreo y Optimizaci√≥n](#-monitoreo-y-optimizaci√≥n)
- [Mejores Pr√°cticas](#-mejores-pr√°cticas)

## üèó Arquitectura del Sistema

```mermaid
graph TB
    subgraph "Cliente"
        A[Aplicaci√≥n/Usuario]
        B[SDK OpenAI/HTTP Client]
    end
    
    subgraph "Azure API Management"
        C[API Gateway]
        D[Pol√≠tica de Cach√© Sem√°ntico]
        E{Router por Operaci√≥n}
        F[Pol√≠tica Embeddings<br/>Threshold: 0.95]
        G[Pol√≠tica Completions<br/>Threshold: 0.10]
    end
    
    subgraph "Cach√© Layer"
        H[(Cach√© Interno APIM)]
        I[B√∫squeda Sem√°ntica]
        J[Almacenamiento con TTL]
        R[(Azure Cache for Redis)]
    end
    
    subgraph "Azure AI Foundry"
        K[AI Foundry Gateway]
        L[Deployment Manager]
        M[text-embedding-3-large]
        N[GPT-4.1]
    end
    
    subgraph "M√©tricas"
        O[Application Insights]
        P[Log Analytics]
        Q[Dashboards]
    end
    
    A --> B
    B --> C
    C --> D
    D --> E
    E -->|Embeddings| F
    E -->|Chat/Completions| G
    
    F --> I
    G --> I
    
    I -->|Cache Hit| H
    I -->|Cache Miss| K
    
    H -->|Redis Lookup| R
    R -->|Return Cached| A
    
    K --> L
    L --> M
    L --> N
    
    M -->|Store Result| J
    N -->|Store Result| J
    J --> H
    H -->|Persist| R
    
    D --> O
    O --> P
    P --> Q
    
    style C fill:#0078D4,stroke:#fff,stroke-width:2px
    style D fill:#FF6B6B,stroke:#fff,stroke-width:2px
    style H fill:#51CF66,stroke:#000,stroke-width:2px,color:#000
    style R fill:#DC382D,stroke:#fff,stroke-width:2px
    style K fill:#FFA94D,stroke:#fff,stroke-width:2px
    style M fill:#845EF7,stroke:#fff,stroke-width:2px
    style N fill:#845EF7,stroke:#fff,stroke-width:2px
```

### Flujo de Datos:

1. **Cliente** env√≠a request a API Management
2. **Pol√≠tica de Cach√©** analiza el tipo de operaci√≥n
3. **B√∫squeda Sem√°ntica** verifica si existe respuesta similar
4. **Redis Lookup**: Consulta en Azure Cache for Redis (meli-testing01)
5. **Cache Hit**: Retorna respuesta desde Redis (<50ms)
6. **Cache Miss**: Forward a Azure AI Foundry
7. **Almacenamiento**: Guarda respuesta en cach√© interno y persiste en Redis
8. **TTL Management**: Redis gestiona expiraci√≥n autom√°tica

## ‚ú® Caracter√≠sticas Principales

### üéØ Optimizaciones por Tipo de Operaci√≥n

| Operaci√≥n | Score Threshold | TTL | Particionamiento | Beneficio |
|-----------|----------------|-----|------------------|-----------|
| **Embeddings** | 0.95 | 30 d√≠as | modelo, tipo, dimensiones, usuario | 95% reducci√≥n en latencia |
| **Chat Completions** | 0.10 | 2 horas | modelo, temperatura, tokens, usuario | 85% reducci√≥n en costos |

### üí° Ventajas Clave

- **Reducci√≥n de Costos**: Evita llamadas redundantes a modelos costosos
- **Mejora de Latencia**: Respuestas instant√°neas desde cach√©
- **Escalabilidad**: Maneja picos de tr√°fico sin impactar el backend
- **Inteligencia**: Detecta consultas sem√°nticamente similares

## üîß Implementaci√≥n desde Azure AI Foundry

### üìù Paso 1: Preparar Azure AI Foundry

**¬øQu√© hace?**: Configura tu proyecto en AI Foundry con los modelos necesarios.

1. **Accede a Azure AI Foundry Studio**
   - Ve a [https://ai.azure.com](https://ai.azure.com)
   - Inicia sesi√≥n con tu cuenta Azure

2. **Crea o selecciona un proyecto**
   ```
   AI Foundry Studio
   ‚îî‚îÄ‚îÄ All resources
       ‚îî‚îÄ‚îÄ + New project
           ‚îú‚îÄ‚îÄ Project name: "semantic-cache-project"
           ‚îú‚îÄ‚îÄ Hub: Selecciona o crea uno nuevo
           ‚îî‚îÄ‚îÄ Create
   ```

3. **Despliega los modelos necesarios**
   - En el men√∫ lateral: **Deployments** ‚Üí **+ Deploy model**
   - Modelo 1: `gpt-4` (nombre: "gpt-4.1")
   - Modelo 2: `text-embedding-3-large` (nombre: "text-embedding-3-large")

### üìù Paso 2: Importar API de AI Foundry en API Management

**¬øQu√© hace?**: Importa la definici√≥n de API de Azure AI Foundry para poder aplicar las pol√≠ticas de cach√©.

Seg√∫n la [documentaci√≥n oficial de Microsoft](https://learn.microsoft.com/en-us/azure/api-management/azure-ai-foundry-api), sigue estos pasos:

1. **En Azure Portal, navega a tu API Management**:
   ```
   Azure Portal
   ‚îî‚îÄ‚îÄ API Management services
       ‚îî‚îÄ‚îÄ tu-instancia-apim
           ‚îî‚îÄ‚îÄ APIs (men√∫ lateral)
               ‚îî‚îÄ‚îÄ + Add API
   ```

2. **Selecciona "Create from Azure resource"**:
   - En las opciones que aparecen, busca y selecciona:
   - **Azure AI Foundry**
   - Descripci√≥n: "Connect API Management services to Azure AI Foundry"

3. **Configura la conexi√≥n con AI Foundry**:
   - **Subscription**: Tu suscripci√≥n de Azure
   - **Resource**: Selecciona tu proyecto de AI Foundry
   - **Display name**: `Azure AI Foundry API`
   - **Name**: `azure-ai-foundry-api`
   - **API URL suffix**: `ai-foundry`
   - **Base URL**: Se autocompletar√° con tu endpoint de AI Foundry
   - **Products**: Starter, Unlimited (o los que tengas configurados)
   - **Gateways**: Managed

4. **Configuraci√≥n de autenticaci√≥n**:
   - **Import method**: ‚úì Use managed identity
   - **User assigned managed identity**: Selecciona si tienes una configurada
   - **Add all AI Foundry operations**: ‚úì Marcado

5. **Click "Create"**

**Qu√© hace autom√°ticamente**:
- ‚úÖ Importa todas las operaciones de OpenAI (chat, completions, embeddings)
- ‚úÖ Configura la autenticaci√≥n con managed identity
- ‚úÖ Establece el backend correcto de AI Foundry
- ‚úÖ Mantiene compatibilidad con SDKs de OpenAI

### üìù Paso 3: Verificar la Importaci√≥n

**¬øQu√© hace?**: Confirma que todas las operaciones se importaron correctamente.

1. **En Azure Portal**, navega a tu API Management:
   ```
   Azure Portal
   ‚îî‚îÄ‚îÄ API Management services
       ‚îî‚îÄ‚îÄ tu-instancia-apim
           ‚îî‚îÄ‚îÄ APIs
               ‚îî‚îÄ‚îÄ Azure AI Foundry API
   ```

2. **Verifica las operaciones**:
   Deber√≠as ver:
   - `POST /deployments/{deployment-id}/chat/completions`
   - `POST /deployments/{deployment-id}/completions`
   - `POST /deployments/{deployment-id}/embeddings`
   - Otras operaciones de OpenAI

### üìù Paso 4: Aplicar Pol√≠tica para Embeddings

**¬øQu√© hace?**: Configura cach√© sem√°ntico optimizado para operaciones de embedding con alta precisi√≥n.

1. **Navega a la operaci√≥n de embeddings**:
   ```
   APIs
   ‚îî‚îÄ‚îÄ Azure AI Foundry API
       ‚îî‚îÄ‚îÄ All operations (vista de lista)
           ‚îî‚îÄ‚îÄ Busca: "Creates embeddings" o "/deployments/{deployment-id}/embeddings"
           ‚îî‚îÄ‚îÄ Click en la operaci√≥n
   ```

2. **Entra al editor de pol√≠ticas**:
   - En la vista de dise√±o de la operaci√≥n
   - En la secci√≥n **"Inbound processing"**
   - Click en el icono **`</>`** (Policy code editor)

3. **Borra todo el contenido existente y pega la pol√≠tica completa**:
   
   **IMPORTANTE**: Copia TODO el contenido del archivo `apim-policy-embeddings-only-v2.xml` que incluye:

   ```xml
   <policies>
       <inbound>
           <base />
           
           <!-- Configurar el backend para embeddings -->
           <set-backend-service id="apim-generated-policy" backend-id="aoai-meli-openai-endpoint" />
           
           <!-- Extraer y validar el request body -->
           <set-variable name="requestBody" value="@(context.Request.Body.As<JObject>(preserveContent: true))" />
           
           <!-- Extraer par√°metros espec√≠ficos de embeddings -->
           <set-variable name="input-type" value="@{
               var body = (JObject)context.Variables[&quot;requestBody&quot;];
               return body[&quot;input_type&quot;]?.ToString() ?? &quot;query&quot;;
           }" />
           
           <!-- ... resto de la pol√≠tica ... -->
           
           <!-- Cach√© Sem√°ntico Optimizado para Embeddings -->
           <azure-openai-semantic-cache-lookup 
               score-threshold="0.95"
               embeddings-backend-id="text-embedding-3-large" 
               embeddings-backend-auth="system-assigned">
               <!-- ... configuraci√≥n de vary-by ... -->
           </azure-openai-semantic-cache-lookup>
       </inbound>
       
       <backend>
           <base />
       </backend>
       
       <outbound>
           <base />
           <!-- TTL de 30 d√≠as para embeddings -->
           <choose>
               <when condition="@(context.Response.StatusCode == 200)">
                   <azure-openai-semantic-cache-store duration="2592000" />
               </when>
           </choose>
           <!-- ... headers de monitoreo ... -->
       </outbound>
       
       <on-error>
           <base />
           <!-- ... manejo de errores ... -->
       </on-error>
   </policies>
   ```

4. **Actualiza el backend-id si es necesario**:
   - Busca: `backend-id="aoai-meli-openai-endpoint"`
   - Reemplaza con tu backend ID real

5. **Click "Save"**

**Caracter√≠sticas clave de esta pol√≠tica**:
- **Score threshold: 0.95** - Solo cachea coincidencias exactas
- **TTL: 30 d√≠as (2592000 segundos)** - Embeddings son determin√≠sticos
- **Particionamiento avanzado**: 
  - Por tipo de input (query/document/passage)
  - Por dimensiones (1536/3072)
  - Por usuario
  - Hash exacto del input
- **Headers de monitoreo**:
  - `X-Semantic-Cache-Status`: HIT/MISS
  - `X-Cache-TTL-Days`: 30
  - `X-Embedding-Type`: query/document/passage
  - `X-Batch-Size`: Para operaciones batch

### üìù Paso 5: Aplicar Pol√≠tica para Chat Completions

**¬øQu√© hace?**: Configura cach√© sem√°ntico flexible para operaciones de chat con threshold bajo.

1. **Navega a la operaci√≥n de chat completions**:
   ```
   APIs
   ‚îî‚îÄ‚îÄ Azure AI Foundry API
       ‚îî‚îÄ‚îÄ All operations
           ‚îî‚îÄ‚îÄ Busca: "Creates a chat completion" o "/deployments/{deployment-id}/chat/completions"
           ‚îî‚îÄ‚îÄ Click en la operaci√≥n
   ```

2. **Entra al editor de pol√≠ticas**:
   - Click en **`</>`** en "Inbound processing"

3. **Borra todo y pega el contenido completo de `apim-policy-completions-only-v2.xml`**:

   ```xml
   <policies>
       <inbound>
           <base />
           
           <!-- Configurar el backend para completions -->
           <set-backend-service id="apim-generated-policy" backend-id="aoai-meli-openai-endpoint" />
           
           <!-- Extraer par√°metros del request -->
           <set-variable name="requestBody" value="@(context.Request.Body.As<JObject>(preserveContent: true))" />
           
           <set-variable name="temperature" value="@{
               var body = (JObject)context.Variables[&quot;requestBody&quot;];
               return body[&quot;temperature&quot;]?.Value<float>() ?? 0.7f;
           }" />
           
           <!-- ... resto de variables ... -->
           
           <!-- Cach√© Sem√°ntico para Completions -->
           <azure-openai-semantic-cache-lookup 
               score-threshold="0.10"
               embeddings-backend-id="text-embedding-3-large" 
               embeddings-backend-auth="system-assigned" 
               max-message-count="20">
               <!-- ... configuraci√≥n de vary-by ... -->
           </azure-openai-semantic-cache-lookup>
       </inbound>
       
       <backend>
           <base />
       </backend>
       
       <outbound>
           <base />
           <!-- TTL fijo de 2 horas -->
           <choose>
               <when condition="@(context.Response.StatusCode == 200)">
                   <azure-openai-semantic-cache-store duration="7200" />
               </when>
           </choose>
           <!-- ... headers de monitoreo ... -->
       </outbound>
       
       <on-error>
           <base />
       </on-error>
   </policies>
   ```

4. **Actualiza el backend-id**

5. **Click "Save"**

**Caracter√≠sticas clave de esta pol√≠tica**:
- **Score threshold: 0.10** - Permite consultas similares
- **TTL: 2 horas (7200 segundos)** - Balance frescura/eficiencia
- **Particionamiento inteligente**:
  - Por grupo de temperatura (deterministic/low/medium/high)
  - Por rango de max_tokens
  - Por usuario
  - Por system message
  - Por funciones/herramientas
- **Headers informativos**:
  - `X-Temperature-Group`: Clasificaci√≥n de temperatura
  - `X-Recommended-TTL-Hours`: TTL sugerido por temperatura
  - `X-Cache-Optimization-Tip`: Consejos de optimizaci√≥n

### üìù Paso 5.1: Aplicar Pol√≠tica para Completions (Opcional)

Si tambi√©n usas el endpoint de completions (no chat):

1. **Navega a**: `/deployments/{deployment-id}/completions`
2. **Aplica la misma pol√≠tica** de completions
3. La pol√≠tica detectar√° autom√°ticamente el tipo de operaci√≥n

### üìù Paso 6: Configurar Backend y Seguridad

**¬øQu√© hace?**: Asegura la conexi√≥n entre API Management y AI Foundry.

1. **En Settings de la API**:
   ```
   Web service URL: https://tu-proyecto.openai.azure.com/openai
   ```

2. **Configurar Managed Identity**:
   - API Management ‚Üí Managed identities ‚Üí System assigned ‚Üí Status: On
   - Copia el Object ID

3. **En AI Foundry**, asigna permisos:
   - Project ‚Üí Access control (IAM)
   - Add role assignment ‚Üí Cognitive Services User
   - Assign to: Managed identity ‚Üí Select your APIM

### üìù Paso 7: Crear Subscription Keys

**¬øQu√© hace?**: Genera claves de acceso para tus aplicaciones.

1. **En API Management ‚Üí Subscriptions**:
   ```
   + Add subscription
   ‚îú‚îÄ‚îÄ Name: production-app
   ‚îú‚îÄ‚îÄ Display name: Production Application
   ‚îú‚îÄ‚îÄ Scope: Azure AI Foundry API
   ‚îî‚îÄ‚îÄ Create
   ```

2. **Obt√©n las claves**:
   - Click en "..." ‚Üí Show/hide keys
   - Copia la Primary key

## üß™ Scripts de Prueba y Validaci√≥n

### üî¨ Test 1: Validaci√≥n de Cach√© de Embeddings

**Archivo**: `test-embedding-cache.py`

**¬øQu√© prueba?**
1. **Exactitud del threshold (0.95)**:
   - Verifica que solo consultas id√©nticas generan HIT
   - Valida que consultas similares generan MISS

2. **Particionamiento correcto**:
   - Diferentes `input_type` (query/document/passage)
   - Diferentes dimensiones (1536/3072)
   - Diferentes usuarios

3. **Batch processing**:
   - Arrays de inputs
   - Validaci√≥n de hash para batches

**Beneficios**:
- ‚úÖ Confirma configuraci√≥n correcta del threshold alto
- ‚úÖ Valida el particionamiento para evitar colisiones
- ‚úÖ Asegura persistencia de 30 d√≠as

**Ejecuci√≥n**:
```bash
python test-embedding-cache.py

# Salida esperada:
‚ñ∂ Test 2/10: Consulta id√©ntica - Debe ser HIT
Resultado:
  ‚îî‚îÄ Cache Status: HIT
  ‚îî‚îÄ Cache Score: 1.0
  ‚îî‚îÄ TTL (d√≠as): 30
  ‚îî‚îÄ Tiempo de respuesta: 0.021s
  ‚îî‚îÄ Validaci√≥n: ‚úì (Esperado: HIT)
```

### üî¨ Test 2: Validaci√≥n de Cach√© de Completions

**Archivo**: `test-completions-cache.py`

**¬øQu√© prueba?**
1. **Flexibilidad del threshold (0.10)**:
   - Consultas similares deben generar HIT
   - "What are Python best practices?" ‚âà "What are the Python best practices?"

2. **Grupos de temperatura**:
   - Determin√≠stica (0.0-0.2): Mayor reuso
   - Baja (0.2-0.5): Reuso moderado
   - Media (0.5-0.8): Reuso limitado
   - Alta (0.8+): M√≠nimo reuso

3. **Par√°metros avanzados**:
   - frequency_penalty y presence_penalty
   - Conversaciones multi-turno
   - Funciones/herramientas

**Beneficios**:
- ‚úÖ Maximiza hit rate con threshold bajo
- ‚úÖ Valida agrupaci√≥n inteligente por temperatura
- ‚úÖ Asegura compatibilidad con features avanzadas

**Ejecuci√≥n**:
```bash
python test-completions-cache.py

# M√©tricas generadas:
üìä Estad√≠sticas Generales:
  ‚îî‚îÄ Hit Rate: 41.7%
  ‚îî‚îÄ Mejora de velocidad: 15.2x

üí∞ Estimaci√≥n de Ahorros:
  ‚îî‚îÄ Ahorro mensual proyectado: $567.30
```

### üî¨ Interpretaci√≥n de Resultados

**Headers de respuesta clave**:

```http
# Para Embeddings
X-Semantic-Cache-Status: HIT
X-Cache-TTL-Days: 30
X-Batch-Size: 5
X-Cache-Optimization-Tip: "Document embeddings cached for 30 days"

# Para Completions  
X-Semantic-Cache-Status: MISS
X-Recommended-TTL-Hours: 12
X-Temperature-Group: deterministic
X-Cache-Optimization-Tip: "Low temperature - consider longer TTL"
```

## üìä Monitoreo y Optimizaci√≥n

### Dashboard Recomendado

1. **Crear dashboard en Azure Portal**:
   ```
   Portal ‚Üí Dashboard ‚Üí + New dashboard
   ‚îî‚îÄ‚îÄ Semantic Cache Monitor
       ‚îú‚îÄ‚îÄ Hit Rate Chart (Line)
       ‚îú‚îÄ‚îÄ Response Time Comparison (Bar)
       ‚îú‚îÄ‚îÄ Cost Savings (KPI)
       ‚îî‚îÄ‚îÄ Top Cached Queries (Table)
   ```

2. **Queries de Application Insights**:
   ```kusto
   // Hit Rate por Hora
   customMetrics
   | where name == "CacheHitRate"
   | summarize avg(value) by bin(timestamp, 1h)
   | render timechart
   ```

### Alertas Cr√≠ticas

1. **Hit Rate Bajo**:
   - Condici√≥n: Hit Rate < 20%
   - Acci√≥n: Revisar threshold y particionamiento

2. **Latencia Alta**:
   - Condici√≥n: P95 > 5 segundos
   - Acci√≥n: Verificar backend y cach√©

## üéØ Mejores Pr√°cticas

### Para Embeddings

```python
# Normalizar texto para maximizar hits
text = text.lower().strip()
text = ' '.join(text.split())  # Normalizar espacios

# Especificar input_type
request = {
    "input": text,
    "input_type": "document",  # Mejora particionamiento
    "dimensions": 3072
}
```

### Para Completions

```python
# Para consultas frecuentes (FAQs)
request = {
    "messages": [...],
    "temperature": 0.1,  # Baja para consistencia
    "seed": 42,         # Reproducibilidad
    "max_tokens": 150   # Limitar variabilidad
}
```

### Monitoreo de Costos

| M√©trica | Sin Cach√© | Con Cach√© | Ahorro |
|---------|-----------|-----------|---------|
| Embeddings/d√≠a | 10,000 √ó $0.0004 = $4 | 6,000 √ó $0.0004 = $2.40 | $1.60 (40%) |
| Completions/d√≠a | 1,000 √ó $0.03 = $30 | 700 √ó $0.03 = $21 | $9 (30%) |
| **Total Mensual** | **$1,020** | **$702** | **$318** |

## üöÄ Pr√≥ximos Pasos

1. **Implementar Redis Cache** para escalabilidad horizontal
2. **Agregar compresi√≥n** para respuestas grandes
3. **Crear SDK cliente** con retry autom√°tico
4. **Implementar warming** de cach√© para consultas comunes
5. **Agregar versionado** de respuestas cacheadas

## üìö Referencias

- [Azure AI Foundry + API Management](https://learn.microsoft.com/en-us/azure/api-management/azure-ai-foundry-api)
- [Semantic Cache Policies](https://learn.microsoft.com/azure/api-management/azure-openai-semantic-cache-lookup-policy)
- [Azure OpenAI Service](https://learn.microsoft.com/azure/ai-services/openai/)