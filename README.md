# Azure API Management Semantic Cache - Embedding-Optimized Implementation

A high-performance semantic cache implementation specifically optimized for Azure OpenAI embedding operations (text-embedding-3-large), with support for other operations. Reduces embedding API costs by up to 99% and improves response times by 100x through intelligent exact-match and similarity-based caching.

## ðŸŽ¯ Key Features for Embeddings

- **Exact Match Caching**: 0.95+ similarity threshold for embedding operations ensures identical inputs return cached results
- **Extended TTL**: 7-14 days cache duration for embeddings (vs 1-12 hours for completions)
- **Batch Optimization**: Efficient caching for batch embedding requests
- **Input Type Awareness**: Different cache strategies for query vs document embeddings
- **Dimension Support**: Handles variable embedding dimensions (256, 1536, 3072)

## Quick Start

### Prerequisites

- Azure API Management instance
- Azure AI Foundry with text-embedding-3-large deployment
- Python 3.8+ (for testing and monitoring)

### 1. Deploy Embedding-Optimized Policy

Apply this policy to your API Management instance:

```xml
<policies>
    <inbound>
        <base />
        <set-backend-service id="apim-generated-policy" backend-id="your-backend-id" />
        
        <!-- Detect operation type -->
        <set-variable name="operation-type" value="@{
            var path = context.Request.Url.Path;
            if (path.Contains(&quot;/embeddings&quot;)) return &quot;embeddings&quot;;
            if (path.Contains(&quot;/chat/completions&quot;)) return &quot;chat&quot;;
            return &quot;other&quot;;
        }" />
        
        <!-- EMBEDDING-OPTIMIZED Semantic Cache -->
        <azure-openai-semantic-cache-lookup 
            score-threshold="@{
                var opType = context.Variables.GetValueOrDefault("operation-type", "");
                return opType == "embeddings" ? "0.95" : "0.10";
            }" 
            embeddings-backend-id="text-embedding-3-large" 
            embeddings-backend-auth="system-assigned" 
            max-message-count="10">
            
            <!-- Cache partitioning optimized for embeddings -->
            <vary-by>@(context.Subscription?.Id ?? "anonymous")</vary-by>
            <vary-by>@(context.Request.MatchedParameters["deployment-id"])</vary-by>
            <vary-by>@{
                if (context.Variables.GetValueOrDefault("operation-type", "") == "embeddings") {
                    var body = context.Request.Body.As<JObject>(preserveContent: true);
                    var inputType = body["input_type"]?.ToString() ?? "query";
                    var dimensions = body["dimensions"]?.ToString() ?? "3072";
                    return $"emb|type:{inputType}|dim:{dimensions}";
                }
                return "other";
            }</vary-by>
        </azure-openai-semantic-cache-lookup>
    </inbound>
    
    <outbound>
        <base />
        <choose>
            <when condition="@(context.Response.StatusCode == 200)">
                <!-- Extended cache duration for embeddings -->
                <azure-openai-semantic-cache-store duration="@{
                    var opType = context.Variables.GetValueOrDefault("operation-type", "");
                    if (opType == "embeddings") {
                        var body = context.Request.Body.As<JObject>(preserveContent: true);
                        var inputType = body["input_type"]?.ToString() ?? "query";
                        return inputType == "document" ? "1209600" : "604800"; // 14 or 7 days
                    }
                    return "3600"; // 1 hour for others
                }" />
            </when>
        </choose>
    </outbound>
</policies>
```

### 2. Configure for Your Deployment

Update these values in the policy:
- `backend-id`: Your AI backend identifier
- `embeddings-backend-id`: Your embedding deployment name

### 3. Test Embedding Cache

```python
import requests
import time
import json

APIM_ENDPOINT = "https://your-apim.azure-api.net/openai"
APIM_KEY = "your-subscription-key"

def test_embedding_cache():
    headers = {
        "Ocp-Apim-Subscription-Key": APIM_KEY,
        "Content-Type": "application/json"
    }
    
    # Test data
    test_input = "Machine learning enables systems to learn from data"
    
    # First call - will miss cache
    start = time.time()
    response1 = requests.post(
        f"{APIM_ENDPOINT}/deployments/text-embedding-3-large/embeddings",
        headers=headers,
        json={
            "input": test_input,
            "input_type": "document",
            "dimensions": 3072
        }
    )
    time1 = time.time() - start
    
    # Second call - should hit cache
    start = time.time()
    response2 = requests.post(
        f"{APIM_ENDPOINT}/deployments/text-embedding-3-large/embeddings",
        headers=headers,
        json={
            "input": test_input,
            "input_type": "document",
            "dimensions": 3072
        }
    )
    time2 = time.time() - start
    
    print(f"First call: {time1:.3f}s - Cache: {response1.headers.get('X-Semantic-Cache-Status')}")
    print(f"Second call: {time2:.3f}s - Cache: {response2.headers.get('X-Semantic-Cache-Status')}")
    print(f"Speed improvement: {time1/time2:.1f}x")

test_embedding_cache()
```

## Embedding-Specific Configuration

### Input Types and Cache Duration

| Input Type | Cache Duration | Use Case |
|------------|----------------|----------|
| `query` | 7 days | Search queries, questions |
| `document` | 14 days | Static documents, knowledge base |
| `passage` | 14 days | Document chunks, paragraphs |

### Similarity Thresholds

| Operation | Threshold | Rationale |
|-----------|-----------|-----------|
| Embeddings | 0.95 | Near-exact match for deterministic results |
| Chat Completions | 0.10 | Semantic similarity for varied responses |
| Completions | 0.15 | Balance between accuracy and cache hits |

### Cache Key Components for Embeddings

The cache key for embeddings includes:
- Subscription ID
- Deployment name (e.g., text-embedding-3-large)
- Input type (query/document/passage)
- Dimensions (256/1536/3072)
- Encoding format (float/base64)
- Input hash (for exact matching)

## Advanced Embedding Scenarios

### 1. Batch Embedding Optimization

```python
# Batch embedding request
def get_batch_embeddings(texts, input_type="document"):
    response = requests.post(
        f"{APIM_ENDPOINT}/deployments/text-embedding-3-large/embeddings",
        headers=headers,
        json={
            "input": texts,  # List of strings
            "input_type": input_type,
            "dimensions": 3072
        }
    )
    return response.json()

# Example: Embed multiple documents
documents = [
    "Azure AI provides powerful language models",
    "Semantic caching improves performance",
    "Embeddings enable similarity search"
]

embeddings = get_batch_embeddings(documents, "document")
```

### 2. Dimension Optimization

```python
# Use smaller dimensions for faster processing
def get_compact_embedding(text):
    response = requests.post(
        f"{APIM_ENDPOINT}/deployments/text-embedding-3-large/embeddings",
        headers=headers,
        json={
            "input": text,
            "dimensions": 256  # Smaller, faster, still accurate
        }
    )
    return response.json()
```

### 3. Document Processing Pipeline

```python
def process_documents_with_cache(documents):
    """Process documents with cache-aware batching"""
    
    results = []
    batch_size = 50  # Optimal batch size
    
    for i in range(0, len(documents), batch_size):
        batch = documents[i:i+batch_size]
        
        response = requests.post(
            f"{APIM_ENDPOINT}/deployments/text-embedding-3-large/embeddings",
            headers=headers,
            json={
                "input": batch,
                "input_type": "document",  # Long cache TTL
                "dimensions": 1536  # Balance size/accuracy
            }
        )
        
        cache_status = response.headers.get('X-Semantic-Cache-Status')
        print(f"Batch {i//batch_size + 1}: Cache {cache_status}")
        
        results.extend(response.json()['data'])
    
    return results
```

## Performance Monitoring

### Custom KQL Queries for Embeddings

```kusto
// Embedding cache performance
ApiManagementGatewayLogs
| where TimeGenerated > ago(24h)
| where OperationName contains "embeddings"
| extend CacheStatus = tostring(ResponseHeaders["X-Semantic-Cache-Status"])
| summarize 
    TotalRequests = count(),
    CacheHits = countif(CacheStatus == "HIT"),
    AvgResponseTime = avg(ResponseTime),
    P95ResponseTime = percentile(ResponseTime, 95)
    by bin(TimeGenerated, 5m)
| extend HitRate = CacheHits * 100.0 / TotalRequests
| project TimeGenerated, HitRate, AvgResponseTime, P95ResponseTime

// Cost analysis for embeddings
ApiManagementGatewayLogs
| where TimeGenerated > ago(7d)
| where OperationName contains "embeddings"
| extend 
    CacheStatus = tostring(ResponseHeaders["X-Semantic-Cache-Status"]),
    InputLength = toint(RequestHeaders["Content-Length"])
| summarize 
    TotalRequests = count(),
    CachedRequests = countif(CacheStatus == "HIT"),
    TotalInputChars = sum(InputLength)
    by bin(TimeGenerated, 1h)
| extend 
    TokensSaved = (CachedRequests * TotalInputChars) / 4, // Rough token estimate
    CostSaved = TokensSaved * 0.0001 / 1000 // $0.0001 per 1K tokens
| project TimeGenerated, CachedRequests, TokensSaved, CostSaved
```

### Python Monitoring Script

```python
import os
from datetime import datetime, timedelta
from azure.monitor.query import LogsQueryClient
from azure.identity import DefaultAzureCredential

def monitor_embedding_cache():
    """Monitor embedding cache performance"""
    
    credential = DefaultAzureCredential()
    client = LogsQueryClient(credential)
    
    workspace_id = os.environ["LOG_ANALYTICS_WORKSPACE_ID"]
    
    query = """
    ApiManagementGatewayLogs
    | where TimeGenerated > ago(1h)
    | where OperationName contains "embeddings"
    | extend CacheStatus = tostring(ResponseHeaders["X-Semantic-Cache-Status"])
    | summarize 
        Total = count(),
        Hits = countif(CacheStatus == "HIT")
    """
    
    response = client.query_workspace(
        workspace_id=workspace_id,
        query=query,
        timespan=timedelta(hours=1)
    )
    
    for row in response.tables[0].rows:
        total, hits = row[0], row[1]
        hit_rate = (hits / total * 100) if total > 0 else 0
        print(f"Embedding Cache - Total: {total}, Hits: {hits}, Rate: {hit_rate:.1f}%")
```

## Best Practices for Embedding Caching

### 1. Input Normalization

```python
def normalize_embedding_input(text):
    """Normalize text for better cache hits"""
    # Remove extra whitespace
    text = " ".join(text.split())
    # Lowercase for queries (not documents)
    # Strip punctuation if appropriate
    return text.strip()
```

### 2. Batch Strategy

```python
def smart_batch_embeddings(texts, cache_client):
    """Batch with cache awareness"""
    
    # Check which texts might be cached
    uncached = []
    cached_results = {}
    
    for text in texts:
        # Try cache first (simplified)
        cached = cache_client.get(text)
        if cached:
            cached_results[text] = cached
        else:
            uncached.append(text)
    
    # Batch request only uncached
    if uncached:
        new_embeddings = get_batch_embeddings(uncached)
        # Combine results
        
    return combined_results
```

### 3. Precompute Common Embeddings

```python
def precompute_knowledge_base():
    """Precompute embeddings during off-peak"""
    
    knowledge_base = load_documents()
    
    for doc in knowledge_base:
        response = requests.post(
            f"{APIM_ENDPOINT}/deployments/text-embedding-3-large/embeddings",
            headers=headers,
            json={
                "input": doc.content,
                "input_type": "document",  # 14-day cache
                "dimensions": 3072,
                "user": "precompute_job"
            }
        )
        
        print(f"Precomputed: {doc.id} - Cache: {response.headers.get('X-Semantic-Cache-Status')}")
```

## Cost Optimization

### Embedding Cost Calculator

```python
def calculate_embedding_savings(requests_per_day, avg_text_length=1000, cache_hit_rate=0.8):
    """Calculate cost savings from caching"""
    
    # Pricing
    tokens_per_text = avg_text_length / 4  # Rough estimate
    cost_per_1k_tokens = 0.0001  # text-embedding-3-large
    
    # Without cache
    total_tokens = requests_per_day * tokens_per_text
    cost_without_cache = (total_tokens / 1000) * cost_per_1k_tokens
    
    # With cache
    cache_misses = requests_per_day * (1 - cache_hit_rate)
    tokens_with_cache = cache_misses * tokens_per_text
    cost_with_cache = (tokens_with_cache / 1000) * cost_per_1k_tokens
    
    # Savings
    daily_savings = cost_without_cache - cost_with_cache
    annual_savings = daily_savings * 365
    
    return {
        "daily_cost_without_cache": f"${cost_without_cache:.2f}",
        "daily_cost_with_cache": f"${cost_with_cache:.2f}",
        "daily_savings": f"${daily_savings:.2f}",
        "annual_savings": f"${annual_savings:.2f}",
        "roi_percentage": f"{(daily_savings/cost_without_cache)*100:.1f}%"
    }

# Example
savings = calculate_embedding_savings(
    requests_per_day=100000,
    avg_text_length=500,
    cache_hit_rate=0.85
)
print(json.dumps(savings, indent=2))
```

## Troubleshooting Embedding Cache

### Common Issues

1. **Low Cache Hit Rate for Embeddings**
   - Check input normalization
   - Verify `input_type` consistency
   - Ensure dimensions match

2. **Cache Not Working**
   ```bash
   # Check headers
   curl -v -X POST https://your-apim.azure-api.net/openai/deployments/text-embedding-3-large/embeddings \
     -H "Ocp-Apim-Subscription-Key: $KEY" \
     -H "Content-Type: application/json" \
     -d '{"input": "test", "input_type": "query"}' \
     2>&1 | grep X-Semantic-Cache
   ```

3. **Performance Issues**
   - Monitor embedding backend latency
   - Check batch sizes (optimal: 50-100)
   - Verify dimension settings

### Debug Headers

| Header | Description | Example |
|--------|-------------|---------|
| `X-Semantic-Cache-Status` | Cache hit/miss | `HIT` |
| `X-Semantic-Cache-Score` | Similarity score | `0.9987` |
| `X-Embedding-Cache-Key` | Cache key used | `emb:text-embedding-3-large|type:document|dim:3072` |
| `X-Cache-TTL-Hours` | Hours until expiry | `336` |

## Production Checklist

- [ ] Configure managed identity for APIM
- [ ] Set up monitoring dashboards
- [ ] Implement input normalization
- [ ] Plan precomputation strategy
- [ ] Configure alerts for cache hit rate < 80%
- [ ] Document dimension choices
- [ ] Test batch processing
- [ ] Validate cache key generation

## Next Steps

1. **Advanced Optimization**
   - Implement intelligent batching
   - Add compression for large embeddings
   - Create tiered caching (memory + disk)

2. **Integration Patterns**
   - RAG system optimization
   - Similarity search acceleration
   - Document processing pipelines

3. **Monitoring Enhancement**
   - Real-time cache analytics
   - Cost tracking dashboard
   - Performance benchmarking

## Support

- [Azure OpenAI Embeddings Guide](https://learn.microsoft.com/azure/ai-services/openai/how-to/embeddings)
- [API Management Semantic Cache](https://learn.microsoft.com/azure/api-management/azure-openai-semantic-cache)
- [text-embedding-3-large Documentation](https://platform.openai.com/docs/guides/embeddings)