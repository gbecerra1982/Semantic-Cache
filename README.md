# üöÄ Cach√© Sem√°ntica con Azure AI Foundry

Una implementaci√≥n de cach√© sem√°ntica inteligente que utiliza Azure AI Foundry para optimizar las llamadas a GPT-4, reduciendo costos y mejorando el rendimiento hasta 20x mediante la detecci√≥n de consultas sem√°nticamente similares.

## üìã Tabla de Contenidos

- [Caracter√≠sticas](#-caracter√≠sticas)
- [Arquitectura](#-arquitectura)
- [Requisitos Previos](#-requisitos-previos)
- [Instalaci√≥n](#-instalaci√≥n)
- [Configuraci√≥n](#-configuraci√≥n)
- [Uso](#-uso)
- [Implementaci√≥n en API Management](#-implementaci√≥n-en-api-management)
- [Monitoreo y M√©tricas](#-monitoreo-y-m√©tricas)
- [Mejores Pr√°cticas](#-mejores-pr√°cticas)

## ‚ú® Caracter√≠sticas

- **üß† Detecci√≥n Sem√°ntica Inteligente**: Identifica consultas similares aunque est√©n escritas de forma diferente
- **‚ö° Mejora de Rendimiento 20x**: Respuestas en ~0.3s vs ~5s en llamadas directas
- **üí∞ Reducci√≥n de Costos**: Evita llamadas redundantes a GPT-4
- **üîÑ Persistencia**: La cach√© se guarda entre ejecuciones
- **üìä M√©tricas Detalladas**: Hit rate, tiempos de respuesta y an√°lisis de similitudes
- **üîå Integraci√≥n con Azure AI Foundry**: Compatible con los √∫ltimos modelos de OpenAI en Azure

## üèó Arquitectura

```mermaid
graph LR
    A[Cliente] --> B[API Management]
    B --> C{Cach√© Sem√°ntica}
    C -->|Cache Hit| D[Redis Cache]
    C -->|Cache Miss| E[Azure AI Foundry]
    E --> F[GPT-4.1]
    E --> G[text-embedding-3-large]
    C --> H[FAISS Index]
```

### Componentes Principales:

1. **Azure AI Foundry**: Plataforma integrada para acceder a modelos de OpenAI
2. **FAISS**: B√∫squeda vectorial eficiente para encontrar similitudes
3. **Redis Cache**: Almacenamiento de respuestas (opcional para producci√≥n)
4. **API Management**: Gesti√≥n de pol√≠ticas y throttling

## üì¶ Requisitos Previos

### Software
- Python 3.8+
- pip (gestor de paquetes de Python)

### Servicios de Azure
- Azure AI Foundry con deployments configurados:
  - GPT-4.1 (para generaci√≥n de respuestas)
  - text-embedding-3-large (para embeddings)
- Azure API Management (opcional para producci√≥n)
- Azure Redis Cache (opcional para escalabilidad)

## üõ† Instalaci√≥n

1. **Clonar o descargar el archivo `SemanticCache.py`**

2. **Instalar dependencias**:
```bash
pip install openai numpy faiss-cpu azure-ai-inference azure-core
```

Para GPU (opcional, mejor rendimiento):
```bash
pip install faiss-gpu
```

## ‚öô Configuraci√≥n

### Credenciales de Azure AI Foundry

El script solicitar√° las siguientes configuraciones al ejecutarse:

```
üîß CONFIGURACI√ìN DE AZURE AI FOUNDRY
============================================================
1. Azure OpenAI SDK con endpoint de OpenAI
2. Azure OpenAI SDK con endpoint de Foundry (RECOMENDADO)
3. Azure AI Foundry SDK (experimental)
```

**Valores predeterminados incluidos**:
- Endpoint: `https://foundry-proyecto1.openai.azure.com/`
- API Key: (se solicitar√° o usa la configurada)
- Deployments:
  - GPT: `gpt-4.1`
  - Embeddings: `text-embedding-3-large`

### Configuraci√≥n de Umbral de Similitud

En el archivo `SemanticCache.py`, puedes ajustar:

```python
SIMILARITY_THRESHOLD = 0.85  # Ajustar entre 0.7 - 0.95
```

- **0.70-0.80**: Captura m√°s variaciones (m√°s cache hits)
- **0.85-0.90**: Balance entre precisi√≥n y cobertura
- **0.90-0.95**: Solo consultas muy similares

## üöÄ Uso

### Ejecuci√≥n B√°sica

```bash
python SemanticCache.py
```

### Flujo de Ejecuci√≥n

1. **Configuraci√≥n Inicial**:
   - Selecciona opci√≥n 2 (Azure OpenAI SDK con Foundry)
   - Presiona Enter para usar valores predeterminados

2. **Pruebas Autom√°ticas**:
   - Ejecuta 11 consultas de prueba
   - Detecta similitudes sem√°nticas
   - Muestra estad√≠sticas en tiempo real

3. **Resultados**:
   ```
   üìà RESUMEN FINAL:
   - Total de consultas: 11
   - Cache hits: 3
   - Cache misses: 8
   - Hit rate final: 27.27%
   - Entradas en cach√©: 8
   ```

### Integraci√≥n en tu Aplicaci√≥n

```python
from SemanticCache import SemanticCache, create_client

# Configuraci√≥n
config = {
    'use_foundry': False,
    'endpoint': 'https://foundry-proyecto1.openai.azure.com/',
    'api_key': 'tu-api-key',
    'gpt_deployment': 'gpt-4.1',
    'embedding_deployment': 'text-embedding-3-large',
    'api_version': '2024-02-01'
}

# Inicializar
chat_client, embedding_client = create_client(config)
cache = SemanticCache(config=config)
cache.load()

# Usar
response = call_gpt_with_cache(prompt, chat_client, cache, config)
```

## üîß Implementaci√≥n en API Management

### Pol√≠tica de Cach√© Sem√°ntica

Crea una pol√≠tica personalizada en Azure API Management:

```xml
<policies>
    <inbound>
        <base />
        <!-- Extraer el prompt del body -->
        <set-variable name="userPrompt" value="@(context.Request.Body.As<JObject>()["messages"].Last["content"].ToString())" />
        
        <!-- Generar hash para cach√© -->
        <cache-lookup-value key="@("embedding-" + context.Variables["userPrompt"].ToString().GetHashCode())" 
                           variable-name="cachedEmbedding" />
        
        <!-- Si no hay embedding en cach√©, generarlo -->
        <choose>
            <when condition="@(context.Variables.ContainsKey("cachedEmbedding") == false)">
                <send-request mode="new" response-variable-name="embeddingResponse">
                    <set-url>https://foundry-proyecto1.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-02-01</set-url>
                    <set-method>POST</set-method>
                    <set-header name="api-key" exists-action="override">
                        <value>{{foundry-api-key}}</value>
                    </set-header>
                    <set-body>@{
                        return new JObject(
                            new JProperty("input", context.Variables["userPrompt"])
                        ).ToString();
                    }</set-body>
                </send-request>
                
                <!-- Guardar embedding en cach√© -->
                <cache-store-value key="@("embedding-" + context.Variables["userPrompt"].ToString().GetHashCode())" 
                                  value="@(((IResponse)context.Variables["embeddingResponse"]).Body.As<JObject>()["data"][0]["embedding"])" 
                                  duration="3600" />
            </when>
        </choose>
        
        <!-- Buscar respuestas similares en cach√© -->
        <set-variable name="similarityThreshold" value="0.85" />
        <!-- Aqu√≠ implementar√≠as la l√≥gica de b√∫squeda vectorial con Redis o Cosmos DB -->
    </inbound>
    
    <backend>
        <base />
    </backend>
    
    <outbound>
        <base />
        <!-- Almacenar respuesta en cach√© si es nueva -->
        <choose>
            <when condition="@(context.Response.StatusCode == 200 && context.Variables.ContainsKey("cacheHit") == false)">
                <cache-store-value key="@("response-" + context.Variables["userPrompt"].ToString().GetHashCode())" 
                                  value="@(context.Response.Body.As<string>())" 
                                  duration="3600" />
            </when>
        </choose>
    </outbound>
    
    <on-error>
        <base />
    </on-error>
</policies>
```

### Configuraci√≥n de Rate Limiting

```xml
<rate-limit-by-key calls="100" renewal-period="60" 
                   counter-key="@(context.Request.Headers.GetValueOrDefault("api-key","anonymous"))" />
```

## üìä Monitoreo y M√©tricas

### Application Insights

Agregar telemetr√≠a personalizada:

```python
from applicationinsights import TelemetryClient

tc = TelemetryClient('your-instrumentation-key')

# En la funci√≥n call_gpt_with_cache
if cached_response:
    tc.track_event('CacheHit', {'prompt': prompt[:50]})
    tc.track_metric('CacheHitRate', cache.get_stats()['hit_rate'])
else:
    tc.track_event('CacheMiss', {'prompt': prompt[:50]})
```

### Consultas KQL √∫tiles

```kusto
// Hit Rate por hora
customEvents
| where name in ("CacheHit", "CacheMiss")
| summarize 
    Hits = countif(name == "CacheHit"),
    Misses = countif(name == "CacheMiss")
    by bin(timestamp, 1h)
| extend HitRate = round(100.0 * Hits / (Hits + Misses), 2)
| project timestamp, HitRate, TotalRequests = Hits + Misses

// Tiempos de respuesta
customMetrics
| where name == "ResponseTime"
| summarize 
    avg(value), 
    percentile(value, 95), 
    percentile(value, 99) 
    by bin(timestamp, 5m)
```

### Dashboard de Azure

1. Crear un nuevo dashboard en Azure Portal
2. Agregar tiles para:
   - Cache Hit Rate (l√≠nea temporal)
   - Tiempo de respuesta promedio
   - Total de requests (con/sin cach√©)
   - Ahorro estimado en tokens

## üéØ Mejores Pr√°cticas

### 1. **Gesti√≥n de Cach√©**

```python
# Limpiar entradas antiguas
def cleanup_old_entries(cache, days=7):
    cutoff_date = datetime.now() - timedelta(days=days)
    # Implementar l√≥gica de limpieza
```

### 2. **Manejo de Errores**

```python
try:
    response = call_gpt_with_cache(prompt, client, cache, config)
except Exception as e:
    logger.error(f"Error en cach√© sem√°ntica: {e}")
    # Fallback a llamada directa
    response = direct_gpt_call(prompt, client)
```

### 3. **Seguridad**

- Nunca hardcodear API keys
- Usar Azure Key Vault para credenciales
- Implementar autenticaci√≥n en API Management

### 4. **Optimizaci√≥n**

- Pre-calcular embeddings para FAQs comunes
- Usar batch processing para m√∫ltiples consultas
- Implementar warming de cach√© en horarios de baja demanda

## üîç Troubleshooting

### Error: "AssertionError" en FAISS
- **Causa**: Dimensiones de embedding incorrectas
- **Soluci√≥n**: El c√≥digo detecta autom√°ticamente las dimensiones

### Error: "Unauthorized" 
- **Causa**: API key o endpoint incorrecto
- **Soluci√≥n**: Verificar credenciales en Azure AI Foundry

### Cache hits bajos
- **Causa**: Umbral muy alto
- **Soluci√≥n**: Reducir `SIMILARITY_THRESHOLD` a 0.75-0.80

## üìà Resultados Esperados

- **Reducci√≥n de latencia**: 85-95% en consultas repetidas
- **Ahorro de costos**: 20-40% dependiendo del patr√≥n de uso
- **Escalabilidad**: Soporta miles de consultas en cach√©

## ü§ù Contribuciones

Para mejorar esta implementaci√≥n:
1. Implementar expiraci√≥n inteligente de cach√©
2. Agregar soporte para m√∫ltiples idiomas
3. Integrar con Azure Cosmos DB para persistencia distribuida

## üìÑ Licencia

Este proyecto es de c√≥digo abierto y est√° disponible bajo la licencia MIT.